Under review as a conference paper at ICLR 2026

Token-validation constraints. At generation time, we constrain the model’s predictions to ensure
that each set of Z code tokens corresponds to a valid item from the catalog. Formally, for each
position (j, £) (code level ¢ of item 7), we restrict the predicted token to the prefix-consistent subset
VO (i; Zj,<e) C {1,...,Cc} and set the logits of any token not in vo. (J; Zj,<¢) to —oo before
the softmax, thereby preventing the model from selecting an invalid code combination. This valida-
tion step ensures that the generated code tuples always decode to legitimate items, which is crucial
for maintaining recommendation feasibility.

4 EXPERIMENT

4.1 EXPERIMENTAL SETTINGS.

Model settings. We use CLHE 2024c) as the item encoder, i.e., E(i) = CLHE(i).

Unless otherwise noted, our RVQ uses L = 4 levels with fixed per-level codebook size Cp = C;
the diffusion horizon is T = ¢ (LZ + 1), where the per-item token length (including the boundary
marker) is denoted by ¢. We utilize a lightweight DDiT architecture for our Diffusion backbone,
with 6 transformer blocks, each with a hidden size of 64 and 8 self-attention heads. The model
operates with a linear noise scheduler a(t) = 1—t. All experiments are performed on four NVIDIA
A40 GPUs, and all models are trained in 20,000 steps.

Datasets. Following prior research on bundle construction [Ma et al.] (2024c); (2025), we
evaluate on two representative datasets, Spotify (Chen et al.) /2018) and POG (Chen et al.) 2019).

Unlike these works, our discrete diffusion model currently requires a fixed number of tokens per in-
stance, so we truncate bundles toa target length. For the Spotify playlist dataset, we create three sub-
sets by capping playlist length at 30/60/90 items (Spotify;,_39 69,99): For the POG fashion dataset,
whose average bundle length is small, we start from its denser variant and derive a fixed-length
version with four items (denoted POG;—4). Unless noted, the input-predict ratio of the bundle,
|b,| : |b,|, are set as 1 : 1, see Table [2] for other settings. Samples shorter than the target length
are dropped. Each dataset is split into train/validation/test with non-overlapping bundles. We also
perform data augmentation by swapping items within the bundle, and the details are describe in
Appendix [B}

Candidate size. To standardize candidate pool, we set a candidate ratio p and construct a shortlist
C of size p|b,| by augmenting the ground-truth targets with randomly sampled non-targets: C =
by U Randomy,_1) ib, |(Z \ b). Unless otherwise stated, we fix p = 100 in all experiments.

4.2 BASELINES.

We consider both non-sequential and sequential construction methods as baselines. To be fair, all
the baselines use the same item features, i.e., pre-trained embeddings via CLHE (Ma et al.| 2024c).

Non-sequential construction methods. They input the partial bundle b, and predict all the items
in the complementary set at once. CLHE (Ma et al.| 2024c): A method that leverages contrastive
learning and hierarchical encoder to learn item and bundle representations. To be noted, CLHE was
not originally designed to predict all the items in the complementary set, while it follows the typical
top-k recommendation paradigm and evaluation protocol. We re-evaluate it against our metrics that
are pertinent to entire bundle construction. BundleNAT (Yang et al. 2024): A non-auto-regressive
generator that predicts a set of items in one shot using preference/compatibility signals. It was
originally used for the task of personalized bundle recommendation instead of bundle construction,
we adapt it for our task by removing the user inputs.

Sequential construction methods. They follow an auto-regressive construction strategy: initialize
So = by; for j = 0,...,|by| — 1, choose 7; = argmaxi¢s, 7(i | sj) and update s;;1 = sj U {a}
until |sj,,|| = |b]. Bi-LSTM (Han et al. 2017): It uses bi-directional LSTM to model the bundle
as a sequence. SASRec (Kang & McAuley} /2018): A Transformer-based sequential recommender
trained for next-item prediction. T]GER (Rajput et al : It generates items as discrete semantic
token sequences with an auto-regressive decoder. BundleMLLM (2025): It finetunes a
multimodal LLM for bundle construction. Its original evaluation is based on the multiple-choice
question protocol since it is impossible to input all the candidate items as input due to context

