Under review as a conference paper at ICLR 2026

Table 1: Overall performance comparison between our DDBC and baselines. ”%Improv.” denotes
the relative improvement over the strongest baseline. Best in bold, second best underlined.

Spotify,, 39 Spotify,,¢9 Spotify,,o9 POGi=4
Model (A/beam) | Fi 4° "Jace? OASt| Flt Jace OASt] FIt Jace} OASt| FIt Jace? OAS tT
CLHE 0.071 0.039 0.373 | 0.100 0.054 0.446 | 0.119 0.065 0.486 |0.140 0.096 0.446
Bi-LSTM 0.124 0.071 0.489 | 0.062 0.034 0.430 | 0.047 0.025 0.426 | 0.035 0.024 0.390
SASRec 0.070 0.043 0.318 | 0.089 0.054 0.310 | 0.050 0.029 0.285 10.169 0.114 0.468
TIGER 0.093 0.053 0.329 | 0.129 0.076 0.413 | 0.123 0.070 0.480 |0.213 0.157 0.546
BundleNAT | 0.153 0.090 0.454 | 0.101 0.056 0.438 | 0.095 0.052 0.446 |0.145 0.097 0.462
BundleMLLM | 0.046 0.024 0.296 | 0.045 0.024 0.324 | 0.052 0.027 0.355 |0.070 0.047 0.322
DDBC 0.282 0.178 0.618 | 0.296 0.185 0.668 | 0.287 0.177 0.684 |0.139 0.098 0.526

Yolmprov. + | 84.3% 97.8% 26.4% | 129.5% 143.4% 49.8% | 133.3% 152.9% 40.7% | —- - -

limitation of LLMs. Even though this setting is easier than our all-ranking setting, to be simple, we
follow this paradigm and set the candidate set as 20.

To be noted, many other recommendation models can be adapted as baselines by following the
paradigm of either the sequential or non-sequential construction. For example, the baselines im-
Heer VGA BY at MultiDAE Gree Pong ee 2018), Hy-
pergraph (Yu et al} /2022), and Transformer (Wei et al.| |2023), etc. or the other advanced sequen-
tial recommendation models. However, we do not include them because they either underperform

CLHE or not highly relevant to the bundle scenario. We implement the most relevant and strongest
baselines to the best of our knowledge, and more baselines could be implemented upon request.

4.3 EVALUATION METRICS

We report retrieval-based metrics F1 and Jaccard (Jacc) (Manning et al. 2008} [Ding et al.| 2023), as
well as a latent-space similarity-based metric OAS (Salton et al.||/1975). Higher F1, Jacc, and OAS

indicate better performance. Let b, denote the set of predicted items, these metrics are calculated
by:

2PR by Ab 1
Fl: PLR Jace : Iby vl OAS := b j S cos(E(a), E(8)), (9)
by Uby| yl (a,B)EM
_ [byAby| _ [bynb,| ys . . . 7
where P = By = pl? and M is the optimal matching between items in b, and by,
Ny 1s
and cos(-,-) denotes cosine similarity. Previous methods in bundle construction use popular next-
item recommendation metrics, such as recall, ndcg, or hit rate [2024c). However, these

metrics are not suitable in the scenario of full bundle construction, which needs to assess the quality
of the predicted entire item set instead of single item. Therefore, we propose these three metrics
to collaboratively measure the performance of bundle construction, offering a comprehensive and
consistent benchmark setting for future studies.

4.4 OVERALL PERFORMANCE COMPARISON

Table [I] shows the overall performance of DDBC compared with baseline methods. First, among
the baselines, BundleNAT and TIGER achieve the strongest performance. These results respec-
tively highlight two key component of our model: the non-sequential construction paradigm and the
advantages of discretizing items into multiple codes. Second, on the Spotify dataset series, DDBC
clearly outperforms all baselines, achieving a 153% improvement in Jacc on Spotify;, 99. Moreover,
the performance gain of DDBC becomes more pronounced as the bundle sequence length increases.
These results demonstrate that DDBC effectively captures the higher-order intra-bundle item rela-
tions, particularly for long-sequence bundles with rich structural dependencies. Third, on POG;—=a,
our model does not outperform TIGER. In fact, since we only predict two items, the task, to some
extent reduces to a next-item prediction scenario, where auto-regressive methods such as TIGER
have a clear advantage.
