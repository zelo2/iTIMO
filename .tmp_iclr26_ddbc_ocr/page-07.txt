Under review as a conference paper at ICLR 2026

Table 2: Effect of input-predict ratio on Spotify,_¢9. Best in bold, second best underlined.

Model 5/55 10/50 30/30 45/15

Flt Jacct OASt] Flt Jacct OASt| FIt Jacct OAST| FIt Jacet OASt
BundleNAT | 0.106 0.059 0.443 | 0.128 0.072 0.463 | 0.101 0.056 0.438 | 0.084 0.046 0.359
SASRec 0.119 0.070 0.425 | 0.131 0.078 0.442 | 0.089 0.054 0.310 | 0.095 0.055 0.315
TIGER 0.087 0.050 0.365 | 0.100 0.059 0.381 | 0.129 0.076 0.413 | 0.154 0.091 0.426
DDBC 0.237 0.144 0.637 | 0.268 0.164 0.664 | 0.296 0.185 0.668 | 0.260 0.161 0.614
Improv. + | 99.2% 105.7% 43.8% | 104.6% 110.3% 43.4% | 129.5% 143.4% 52.5% | 68.8% 76.9% 44.1%

Table 3: Effect of candidate ratio on Spotify,,_¢9. Best in bold, second best underlined.

Model p=10 p=20 p=50 p=100
Flt Jacct OAS*} FI  Jacet OASt| FI* Jacet OAST| FI*  Jacc+ OAS*
BundleNAT | 0.266 0.163 0.508 | 0.210 0.124 0.485 | 0.153 0.088 0.464 | 0.101 0.056 0.438
SASRec 0.292 0.194 0.519 | 0.200 0.126 0.474 | 0.200 0.126 0.475 | 0.089 0.054 0.310
TIGER 0.191 0.151 0.326 | 0.107 0.080 0.295 | 0.108 0.081 0.296 | 0.129 0.076 0.413
DDBC 0.599 0.447 0.763 | 0.503 0.355 0.727 | 0.380 0.250 0.689 | 0.296 0.185 0.668
Improv. + | 105.1% 130.4% 47.0% | 139.5% 181.7% 49.9% | 90.0% 98.4% 45.1% | 129.5% 143.4% 52.5%

4.5 MODEL STUDY

Effect of input-predict ratio. We conduct experiments with different input-predict ratio on
Spotify,—¢9 and report results in Table 2 We observe that DDBC outperforms all baselines across
different partial bundle sizes and exhibits a relatively consistent performance, demonstrating its ro-

Efficiency analysis. We record the inference
time and paremeter size of DDBC and the base-
ine methods, as reported in Figure 2| where

the circle radius indicates each model’s overall
performance. The inference time is measured
on Spotify,—¢9- Specifically, although Bi-
LSTM has fast inference and smallest param-
eters, its performance is not competitive (see
Table [I). DDBC is highly parameter-efficient,
containing only 0.79M parameters, and is sig-
nificantly smaller than other baselines. More-
over, DDBC’s inference speed is comparable
to the one-shot generation method BundleNAT
and faster than all other baseline models; in par-
ticular, it is substantially faster than BundleM-
LLM, which relies on interactions with large

language models.

bustness in scenarios with limited known items. Specifically, when the known partial bundles are
small (e.g., 5/55, 10/50, 30/30), DDBC achieves substantial improvements over the best baseline
by 106%, 110%, 143% on Jaccard, respectively. Although the performance gap narrows down as
the number of input items grow, our method continues to maintain a leading position. These re-
sults highlight that DDBC is capable of generating coherent and distribution-aware bundles even
when only a small subset of items is provided, validating the effectiveness of our masked denoising
ormulation and the discrete diffusion mechanism.

Effect of candidate ratio. In addition, we report the results for DDBC and the baselines under dif-
erent candidate ratios in TableB] The results indicate that while the absolute values of the evaluation
metrics fluctuate as the candidate ratio () increases, the relative improvements of DDBC over all
baselines remain consistently substantial. Interestingly, among baselines, when p increases, TIGER
start to bypass other baselines on F1 and Jacc (p=100). This can be attributed to the fact that RVQ
allows precise reconstruction of item IDs, implying the advantages of using RVQ. These findings
highlight the adaptability of DDBC under varying candidate pool sizes, demonstrating its ability to
maintain strong bundle representations even when the retrieval space becomes more challenging.

7.5 x10"

z BundleMLLm
5 7x10 i
a
Fos x10 oa
E
= os 4
Fi
° 60 - CLHE
g y 7
z
8 aol | TIGER
2 é NLL SaRec
= ao] | ppad “f puridleNaT
‘Best. Se
° 10" io io? 10 io

Parameter size [million]

Figure 2: Illustration of the model efficiency com-
parison. The x-axis is parameter size (millions),
y-axis is inference time (milliseconds per bundle),
and the bubble radius corresponds to overall per-
formance (larger is better).

8
