Under review as a conference paper at ICLR 2026

Algorithm 3 OAS via Hungarian Algorithm (maximize sum of cosine similarities)

Require: Predicted set by = fir, wee sta} (duplicates removed); ground-truth set by = {i1,..., in}; embed-

dings E(-)

Ensure: Optimal matching M C {1,...,A} x {1,...,n} and OAS

1:
2m <— max(f,n)
: build square cost: C €¢ R™*™ > convert max-sim to min-cost

build similarity: 5 ¢ R"*" with S[a, 6] < cos(E(ia), E(is))

for a= 1 to mdo
for b = 1to mdo
ifa < and b <n then
Cla, b] <1 — S{a, b| > cost € [0, 2] since cos € [—1, 1]
else
Cla, b} -1 > dummy pairs have similarity 0
end if
end for

: end for . . .

: row reduction: C[a,-] < C[a,-] — ming Ca, b] for all a

: column reduction: C[-,b] <— C[-,] — ming C[a, b] for all b
: repeat

Cover all zeros in C by the minimum number of horizontal/vertical lines
if (#lines<_m) then _

A+ min{C{a, b] : C{a, b] is uncovered}

Subtract A from every uncovered entry

Add A to every doubly-covered entry

(singly-covered entries unchanged)
end if

: until #lines = m
: extract assignment: find m independent zeros (no two share a row/column) to form an optimal assignment

M C{i,...,m}?

: restrict to real items: M < {(a,b)€ M:a<f, b<n}

2 Sum — Dasyear Sle; 4]

:OASe > denominator is |by| = n
: return V/, ‘OAS

Cc

ADDITIONAL RESULTS

Comparison across datasets using Jaccard. We compare the results crosss datasets using Jaccard
with multiple attempts. As Table [7] shows, among the established baselines, BundleNAT generally
achieves the best Jaccard performance across the Spotify datasets. This suggests that BundleNAT’s
non-auto-regressive architecture is particularly effective at generating relevant set-based results com-
pared to the sequential models. What’s more, our proposed method, DDBC, consistently and sig-
nificantly outperforms all baselines across every dataset and attempt level. On Spotify,_39 , the
DDBC model achieves a Jaccard@1 of 0.164, nearly doubling the performance of the best baseline
(BundleNAT at 0.090). This performance gap confirms the efficacy and advanced capability of our
model, especially when generating predictions with multiple attempts.

Table 7: Comparison across datasets using Jaccard with A € {1, 5,20}. Best in bold, second best
underlined.

Model (A/beam) |

Spotify,,—3o | Spotify,¢o | Spotify, oo | POGk=4

|Jacc@1 Jacc@5 Jacc@20| Jacc@1 Jacc@5 Jacc@20|Jacc@1 Jacc@5 Jacc@20 | Jacc@1 Jacc@5 Jacc@20

CLHE 009 005 002 008 004 002 007 003 .001 059 048 .020
SASRec 043 023 009 054 .030 013 029 013 005 14 066 024
TIGER 053 028 O11 076 .036 013 070 034 013 157 094 038
BundleNAT | 090 076 033 | .056 055 027 | 052 052 026 | .097 055 .023
DDBC | .164 130 053 | 185 137 055 | .177 132 054 | .098 073 032

Latent-space quality. To better explore the latent-space quality of the items generated by our
method, we report the OAS metric at A = 50, as shown in Table 8} For the Spotify dataset series,

15
