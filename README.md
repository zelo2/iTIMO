<p align="center">
  <img src="figures/iTIMO_cropped.png" alt="iTIMO" width="720" />
</p>

# ğŸ„ iTIMO: An LLM-Empowered Synthesis Dataset for Travel Itinerary Modification

This repository provides the dataset and code for *iTIMO: An LLM-Empowered Synthesis Dataset for Travel Itinerary Modification*.

## ğŸ“¦ Dataset

The released benchmark dataset is under `benchmark/iTIMO_dataset/`:
- `benchmark/iTIMO_dataset/iTIMO-Florence/`
- `benchmark/iTIMO_dataset/iTIMO-Melbourne/`
- `benchmark/iTIMO_dataset/iTIMO-Toronto/`

### ğŸ” Perturbation vs. Modification (Important)

In filenames like `benchmark/iTIMO_dataset/iTIMO-Florence/Florence_ADD_test.json`, the `ADD/DELETE/REPLACE` token refers to the **perturbation** operation used to create the need-to-modify itinerary. The **modification** operation is the *inverse*:
- `*_ADD_*.json` â†’ modify with **DELETE** (gold label field: `removed_index`)
- `*_DELETE_*.json` â†’ modify with **ADD** (gold label fields: `insert_index`, `selected_poi`, `selected_cand_id`)
- `*_REPLACE_*.json` â†’ modify with **REPLACE** (gold label fields: `replaced_index`, `selected_poi`, `selected_cand_id`)

### ğŸ§¾ File Naming and Format

- Naming: `<City>_<PerturbOp>_<split>.json` (e.g., `Florence_ADD_test.json`)
- Each file is a JSON dict: `{ "<sid>": sample, ... }`
- `sample["example_input"]` includes:
  - `need_to_modify itinerary`: `[[name, category, lon, lat, popularity], ...]`
  - `hint`: natural-language constraints for axes (popularity / category / spatial)
  - `threshold_low`, `threshold_high`: spatial thresholds (km)
  - `Candidate POIs`: present in `*_DELETE_*.json` and `*_REPLACE_*.json` (needed for ADD/REPLACE modification); typically absent in `*_ADD_*.json`

### ğŸ“Š Dataset Size (#samples)

The dataset statistics are provided in the paper (Table 2):

<p align="center">
  <img src="figures/dataset_stats_table2.png" width="720" alt="iTIMO dataset statistics (Table 2)" />
</p>

## ğŸ§­ Project Structure

This repo has two main parts:
- Data construction & perturbation: [Dataset_Pipline/README.md](Dataset_Pipline/README.md)
- Benchmark & evaluation: [benchmark/README.md](benchmark/README.md)

## ğŸ› ï¸ Installation

Recommended Python `>=3.10`.

```bash
pip install -r requirements.txt
```

Note: running `Dataset_Pipline/V31FM_perturbation.py` / `Dataset_Pipline/baseline_perturbation.py` / `benchmark/Prompting_LLM.py` requires access to the corresponding APIs (DeepSeek / Azure OpenAI / OpenAI, etc.).

## ğŸ§ª Data Construction (Perturbation + Examples)

See [Dataset_Pipline/README.md](Dataset_Pipline/README.md) for perturbation and dataset construction steps.

## ğŸ“ˆ Benchmark & Evaluation

See [benchmark/README.md](benchmark/README.md) for evaluation, inference, parsing, and fine-tuning.

## ğŸ—‚ï¸ Repository Layout (What Each Part Does)

```text
iTIMO/
â”œâ”€â”€ Dataset_Pipline/
â”‚   â”œâ”€â”€ V31FM_perturbation.py â€” main perturbation generator (LLM + tool-calling + optional memory)
â”‚   â”œâ”€â”€ baseline_perturbation.py â€” baseline perturbation generator
â”‚   â”œâ”€â”€ position_POI_extraction.py â€” diff detector between original and perturbed itineraries
â”‚   â”œâ”€â”€ data_cons.py â€” data construction utilities shared across RAG scripts
â”‚   â”œâ”€â”€ dataset.py â€” prompt dataset loader for perturbation outputs
â”‚   â”œâ”€â”€ split_florence.py â€” generate train/val/test CSV splits (7:1:2)
â”‚   â”œâ”€â”€ RAG_build_emd.py â€” RAG data construction with embedding neighbors
â”‚   â”œâ”€â”€ RAG_build_hint.py â€” RAG data construction with hint neighbors
â”‚   â””â”€â”€ template/
â”‚       â”œâ”€â”€ prompts.py â€” prompts for V31FM_perturbation.py
â”‚       â”œâ”€â”€ baseline_prompts.py â€” prompts for baseline_perturbation.py
â”‚       â”œâ”€â”€ functions.py â€” tool JSON schemas for tool-calling
â”‚       â””â”€â”€ CaseStudy.py â€” small demo/case-study helpers
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ Prompting_LLM.py â€” prompt-based itinerary modification runner (Azure/OpenAI/DeepSeek/LM Studio)
â”‚   â”œâ”€â”€ process_pred.py â€” parse model outputs
â”‚   â”œâ”€â”€ eval.py â€” compute accuracy + hint metrics
â”‚   â”œâ”€â”€ hint_satis_check.py â€” per-sample hint satisfaction checker
â”‚   â”œâ”€â”€ benchmark_prompts.py â€” prompt templates for modification tasks
â”‚   â”œâ”€â”€ fine_tune_full.py â€” full-parameter SFT runner
â”‚   â”œâ”€â”€ fine_tune_lora.py â€” LoRA/QLoRA SFT runner
â”‚   â”œâ”€â”€ api_key/api_key.py â€” API key placeholders
â”‚   â””â”€â”€ iTIMO_dataset/ â€” released benchmark splits (train/val/test for each city/op)
â”œâ”€â”€ data4perturb/ â€” Florence LearNext CSVs used by perturbation scripts
â”œâ”€â”€ og_dataset/ â€” raw trajectory/POI datasets (CIKMâ€™16, IJCAIâ€™15)
â”œâ”€â”€ figures/ â€” images used in README
â””â”€â”€ requirements.txt â€” Python dependencies
```
